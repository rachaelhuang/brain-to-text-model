model:
  n_input_features: 512
  n_units: 768
  rnn_dropout: 0.4
  rnn_trainable: true
  n_layers: 5
  patch_size: 14
  patch_stride: 4
  input_network:
    n_input_layers: 1
    input_layer_sizes:
      - 512
    input_trainable: true
    input_layer_dropout: 0.2

gpu_number: '-1'  # CPU
mode: train
use_amp: false  # Disable for CPU
output_dir: finetuned_2025_model
checkpoint_dir: finetuned_2025_model/checkpoint

# Start from pretrained checkpoint
init_from_checkpoint: true
init_checkpoint_path: data/t15_pretrained_rnn_baseline/checkpoint/best_checkpoint

save_best_checkpoint: true
save_all_val_steps: false
save_final_model: true
save_val_metrics: true
early_stopping: true
early_stopping_val_steps: 10  # stop after 10 steps without improvement

# short training (fine-tuning)
num_training_batches: 2000  # short for RT
batches_per_val_step: 200   # Validate frequently
batches_per_train_log: 50

# Lower learning rate for fine-tuning
lr_scheduler_type: cosine
lr_max: 0.0005 
lr_min: 0.00001
lr_decay_steps: 2000
lr_warmup_steps: 100

lr_max_day: 0.0005
lr_min_day: 0.00001
lr_decay_steps_day: 2000
lr_warmup_steps_day: 100

beta0: 0.9
beta1: 0.999
epsilon: 0.1
weight_decay: 0.001
weight_decay_day: 0
seed: 10
grad_norm_clip_value: 10

log_individual_day_val_PER: true
log_val_skip_logs: false
save_val_logits: false
save_val_data: false

dataset:
  data_transforms:
    white_noise_std: 1.0
    constant_offset_std: 0.2
    random_walk_std: 0.0
    random_walk_axis: -1
    static_gain_std: 0.0
    random_cut: 3
    smooth_kernel_size: 100
    smooth_data: true
    smooth_kernel_std: 2
    temporal_mask_enabled: true
    temporal_mask_prob: 0.15
    temporal_mask_length: 4
  
  neural_dim: 512
  batch_size: 32  # We made this smaller for CPU
  n_classes: 41
  max_seq_elements: 500
  days_per_batch: 2  # also smaller for CPU
  seed: 1
  num_dataloader_workers: 2  # Fewer workers for CPU
  loader_shuffle: true
  must_include_days: null
  test_percentage: 0.15
  feature_subset: null
  dataset_dir: ../data/hdf5_data_final
  bad_trials_dict: null
  
  # ONly 2025 sessions for fine-tuning because we noticed it had more temporal degradation
  sessions:
    - t15.2025.01.10
    - t15.2025.01.12
    - t15.2025.03.14
    - t15.2025.03.16
    - t15.2025.03.30
    - t15.2025.04.13
  
  dataset_probability_val:
    - 1
    - 1
    - 1
    - 1
    - 1
    - 1